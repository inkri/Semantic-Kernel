{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e523a5-3222-428c-b901-f2f128e3c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://learn.microsoft.com/en-us/semantic-kernel/concepts/kernel?pivots=programming-language-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f952ffa2-be99-4fd3-b280-84b8753ce944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renewable energy refers to energy sources that are naturally replenished on a human timescale. These sources are virtually inexhaustible, unlike fossil fuels like coal, oil, and natural gas. Here are some key aspects of renewable energy:\n",
      "\n",
      "1. **Types of Renewable Energy**:\n",
      "   - **Solar Energy**: Harnessed from the sun's radiation using technologies like solar panels (photovoltaic cells) or concentrated solar\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.mistral_ai import MistralAIChatCompletion\n",
    "from semantic_kernel.prompt_template.prompt_template_config import PromptTemplateConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import asyncio  # Import asyncio\n",
    "\n",
    "# Initialize the Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Set up the Mistral AI service for chat completion\n",
    "chat_completion = MistralAIChatCompletion(\n",
    "    ai_model_id=\"mistral-small-latest\",\n",
    "    api_key=\"Md5ifgn7nuQjtrOOsx69vlP00X3g2lzr\",\n",
    "    service_id=\"chatbot\"\n",
    ")\n",
    "\n",
    "# Register the service\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# Initialize SentenceTransformer model for text embeddings\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get text embeddings\n",
    "def get_text_embeddings(sentences):\n",
    "    return model.encode(sentences)\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "db_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = db_client.get_or_create_collection(name=\"rag_documents\")\n",
    "\n",
    "# Function to add documents to the vector DB\n",
    "def add_to_vector_db(texts, ids):\n",
    "    embeddings = get_text_embeddings(texts)\n",
    "    collection.add(\n",
    "        ids=[str(i) for i in ids],\n",
    "        embeddings=embeddings,\n",
    "        metadatas=[{\"text\": text} for text in texts]\n",
    "    )\n",
    "\n",
    "# Function to retrieve similar texts from the vector DB\n",
    "def retrieve_similar_texts(query, top_k=1):  # Set top_k to 1 for single retrieval\n",
    "    query_embedding = get_text_embeddings([query])[0]\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"metadatas\"]\n",
    "    )\n",
    "    if results and results[\"metadatas\"]:\n",
    "        return [metadata[\"text\"] for metadata in results[\"metadatas\"][0]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Define prompt settings\n",
    "prompt_setting = kernel.get_prompt_execution_settings_from_service_id(\"chatbot\")\n",
    "prompt_setting.max_tokens = 100\n",
    "prompt_setting.temperature = 0.5\n",
    "\n",
    "# Sample texts and IDs\n",
    "sample_texts = [\n",
    "    \"Renewable energy is derived from natural processes that are replenished constantly.\",\n",
    "    \"Solar energy is a type of renewable energy that harnesses the power of the sun.\",\n",
    "    \"Wind energy uses wind turbines to convert kinetic energy into electricity.\"\n",
    "]\n",
    "sample_ids = [1, 2, 3]\n",
    "add_to_vector_db(sample_texts, sample_ids)\n",
    "\n",
    "# User input query\n",
    "user_input = \"Tell me about renewable energy.\"\n",
    "\n",
    "# Retrieve similar text to the user query\n",
    "retrieved_texts = retrieve_similar_texts(user_input, top_k=1)\n",
    "\n",
    "# Check if any text is retrieved and pass it to the LLM for an answer\n",
    "if retrieved_texts:\n",
    "    prompt_text = f\"Given the following information, please answer the question: {user_input}\\n\\n{retrieved_texts[0]}\"\n",
    "    \n",
    "    # Define the prompt template with retrieved text\n",
    "    prompt_template = PromptTemplateConfig(\n",
    "        template=prompt_text,\n",
    "        name=\"Test1\",\n",
    "        template_format=\"semantic-kernel\",\n",
    "        execution_settings=prompt_setting\n",
    "    )\n",
    "\n",
    "    # Register the prompt function\n",
    "    prompt_function = kernel.add_function(\n",
    "        function_name=\"prompt_function\",\n",
    "        plugin_name=\"semantic_kernel_plugin\",\n",
    "        prompt_template_config=prompt_template\n",
    "    )\n",
    "\n",
    "    # Use await directly to invoke the async function\n",
    "    async def get_llm_answer():\n",
    "        result = await kernel.invoke(prompt_function)\n",
    "        print(result)\n",
    "\n",
    "    # Run the async function within the notebook environment\n",
    "    await get_llm_answer()\n",
    "\n",
    "else:\n",
    "    print(\"No similar text found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae551dc4-97f0-4935-980b-7b2cccab6a8d",
   "metadata": {},
   "source": [
    "## Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25caa69f-723f-48aa-967e-beeba9bcb94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8190a-cb58-4bfe-b5b0-f867edfc4a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da260409-74ea-4fcb-ae01-a75e6b76918c",
   "metadata": {},
   "source": [
    "# Setup of KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37ffbf04-5bf5-4dd3-b91e-cecff30fcbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully added to the ChromaDB vector database.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Initialize SentenceTransformer model for text embeddings\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize ChromaDB client (persistent storage)\n",
    "db_client = chromadb.PersistentClient(path=\"./chroma_db\")  # Specify the path to save the ChromaDB\n",
    "collection = db_client.get_or_create_collection(name=\"rag_documents\")\n",
    "\n",
    "# Function to get text embeddings using SentenceTransformer\n",
    "def get_text_embeddings(sentences):\n",
    "    return model.encode(sentences)\n",
    "\n",
    "# Function to add texts and their embeddings to ChromaDB\n",
    "def add_to_vector_db(texts, ids):\n",
    "    embeddings = get_text_embeddings(texts)  # Create embeddings for texts\n",
    "    collection.add(\n",
    "        ids=[str(i) for i in ids],  # IDs must be strings\n",
    "        embeddings=embeddings,\n",
    "        metadatas=[{\"text\": text} for text in texts]  # Attach original text as metadata\n",
    "    )\n",
    "\n",
    "# Sample knowledge base data (expand as needed)\n",
    "sample_texts = [\n",
    "    \"Renewable energy is derived from natural processes that are replenished constantly.\",\n",
    "    \"Solar energy is a type of renewable energy that harnesses the power of the sun.\",\n",
    "    \"Wind energy uses wind turbines to convert kinetic energy into electricity.\",\n",
    "    \"Geothermal energy utilizes heat from beneath the earth's surface.\",\n",
    "    \"Hydropower generates electricity from the energy of flowing water.\",\n",
    "    \"Biomass energy comes from organic materials such as plants and waste.\",\n",
    "    \"Nuclear energy is produced by splitting atoms in a controlled environment.\",\n",
    "    \"Electric cars are powered by electricity instead of gasoline.\",\n",
    "    \"Energy storage technologies help store excess energy for later use.\",\n",
    "    \"Sustainability focuses on meeting the needs of the present without compromising future generations.\"\n",
    "]\n",
    "\n",
    "sample_ids = [i for i in range(1, len(sample_texts) + 1)]  # Generate unique IDs for each sample text\n",
    "\n",
    "# Add sample data to the vector DB\n",
    "add_to_vector_db(sample_texts, sample_ids)\n",
    "\n",
    "print(\"Data has been successfully added to the ChromaDB vector database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4c523-224a-4e74-9189-43bd00d3cefa",
   "metadata": {},
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f1f7643-ff00-4c96-90be-af8880b7faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Processing User Query...\n",
      "\n",
      "üîç Retrieved Text:\n",
      "Sustainability focuses on meeting the needs of the present without compromising future generations.\n",
      "\n",
      "üìå Arguments:  {'retrieved_text': 'Sustainability focuses on meeting the needs of the present without compromising future generations.', 'user_query': 'Hi'}\n",
      "üìå Prompt Function:  metadata=KernelFunctionMetadata(name='prompt_function', plugin_name='semantic_kernel_plugin', description=None, parameters=[], is_prompt=True, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='The completion result', default_value=None, type_='FunctionResult', is_required=True, type_object=None, schema_data=None, include_in_function_choices=True), additional_properties=None) invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000027B91F22910> streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000027B8BCB7FD0> prompt_template=KernelPromptTemplate(prompt_template_config=PromptTemplateConfig(name='Test1', description='', template='Chat History:\\n    <chat_history><message role=\"user\"><text>Hi</text></message></chat_history>\\n    \\n    Answer the following question based on the provided text within 30 words: \\n    Sustainability focuses on meeting the needs of the present without compromising future generations.\\n    \\n    Question: Hi\\n    \\n    Answer:', template_format='semantic-kernel', input_variables=[], allow_dangerously_set_content=False, execution_settings={'chatbot': MistralAIChatPromptExecutionSettings(service_id='chatbot', extension_data={'ai_model_id': 'mistral-small-latest'}, function_choice_behavior=None, ai_model_id='mistral-small-latest', response_format=None, messages=None, safe_mode=False, safe_prompt=False, max_tokens=100, seed=None, temperature=0.5, top_p=None, random_seed=None, presence_penalty=None, frequency_penalty=None, n=None, retries=None, server_url=None, timeout_ms=None, tools=None, tool_choice=None)}), allow_dangerously_set_content=False) prompt_execution_settings={'chatbot': MistralAIChatPromptExecutionSettings(service_id='chatbot', extension_data={'ai_model_id': 'mistral-small-latest'}, function_choice_behavior=None, ai_model_id='mistral-small-latest', response_format=None, messages=None, safe_mode=False, safe_prompt=False, max_tokens=100, seed=None, temperature=0.5, top_p=None, random_seed=None, presence_penalty=None, frequency_penalty=None, n=None, retries=None, server_url=None, timeout_ms=None, tools=None, tool_choice=None)}\n",
      "\n",
      "ü§ñ AI Response:\n",
      "Hello! How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Tell me about renewable energy sources.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Processing User Query...\n",
      "\n",
      "üîç Retrieved Text:\n",
      "Solar energy is a type of renewable energy that harnesses the power of the sun.\n",
      "\n",
      "üìå Arguments:  {'retrieved_text': 'Solar energy is a type of renewable energy that harnesses the power of the sun.', 'user_query': 'Tell me about renewable energy sources.'}\n",
      "üìå Prompt Function:  metadata=KernelFunctionMetadata(name='prompt_function', plugin_name='semantic_kernel_plugin', description=None, parameters=[], is_prompt=True, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='The completion result', default_value=None, type_='FunctionResult', is_required=True, type_object=None, schema_data=None, include_in_function_choices=True), additional_properties=None) invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000027B8BF68750> streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000027B868F76D0> prompt_template=KernelPromptTemplate(prompt_template_config=PromptTemplateConfig(name='Test1', description='', template='Chat History:\\n    <chat_history><message role=\"user\"><text>Hi</text></message><message role=\"assistant\"><text>Hello! How can I assist you today?</text></message><message role=\"user\"><text>Tell me about renewable energy sources.</text></message></chat_history>\\n    \\n    Answer the following question based on the provided text within 30 words: \\n    Solar energy is a type of renewable energy that harnesses the power of the sun.\\n    \\n    Question: Tell me about renewable energy sources.\\n    \\n    Answer:', template_format='semantic-kernel', input_variables=[], allow_dangerously_set_content=False, execution_settings={'chatbot': MistralAIChatPromptExecutionSettings(service_id='chatbot', extension_data={'ai_model_id': 'mistral-small-latest'}, function_choice_behavior=None, ai_model_id='mistral-small-latest', response_format=None, messages=None, safe_mode=False, safe_prompt=False, max_tokens=100, seed=None, temperature=0.5, top_p=None, random_seed=None, presence_penalty=None, frequency_penalty=None, n=None, retries=None, server_url=None, timeout_ms=None, tools=None, tool_choice=None)}), allow_dangerously_set_content=False) prompt_execution_settings={'chatbot': MistralAIChatPromptExecutionSettings(service_id='chatbot', extension_data={'ai_model_id': 'mistral-small-latest'}, function_choice_behavior=None, ai_model_id='mistral-small-latest', response_format=None, messages=None, safe_mode=False, safe_prompt=False, max_tokens=100, seed=None, temperature=0.5, top_p=None, random_seed=None, presence_penalty=None, frequency_penalty=None, n=None, retries=None, server_url=None, timeout_ms=None, tools=None, tool_choice=None)}\n",
      "\n",
      "ü§ñ AI Response:\n",
      "Renewable energy sources include solar, wind, hydro, geothermal, and biomass, which are naturally replenished and sustainable.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.mistral_ai import MistralAIChatCompletion\n",
    "from semantic_kernel.prompt_template.prompt_template_config import PromptTemplateConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.contents import ChatMessageContent, TextContent\n",
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "from semantic_kernel.core_plugins import TimePlugin \n",
    "\n",
    "# Initialize the Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Set up the AI model (Mistral AI)\n",
    "chat_completion = MistralAIChatCompletion(\n",
    "    ai_model_id=\"mistral-small-latest\",\n",
    "    api_key=\"Md5ifgn7nuQjtrOOsx69vlP00X3g2lzr\",\n",
    "    service_id=\"chatbot\"\n",
    ")\n",
    "\n",
    "# Register the service\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# Initialize SentenceTransformer model for text embeddings\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def get_text_embeddings(sentences):\n",
    "    return model.encode(sentences)\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "db_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = db_client.get_or_create_collection(name=\"rag_documents\")\n",
    "\n",
    "def retrieve_similar_texts(query, top_k=1):\n",
    "    query_embedding = get_text_embeddings([query])[0]\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"metadatas\"]\n",
    "    )\n",
    "    if results and results[\"metadatas\"]:\n",
    "        return [metadata[\"text\"] for metadata in results[\"metadatas\"][0]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "# Import Time Plugin\n",
    "time_plugin = TimePlugin()\n",
    "\n",
    "# Define prompt settings\n",
    "prompt_setting = kernel.get_prompt_execution_settings_from_service_id(\"chatbot\")\n",
    "prompt_setting.max_tokens = 100\n",
    "prompt_setting.temperature = 0.5\n",
    "\n",
    "# Initialize Chat History\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "async def process_user_query(user_input):\n",
    "    print(\"\\nüöÄ Processing User Query...\\n\")\n",
    "    \n",
    "    # Retrieve similar text from the vector database\n",
    "    retrieved_texts = retrieve_similar_texts(user_input)\n",
    "    \n",
    "    if not retrieved_texts:\n",
    "        print(\"‚ö†Ô∏è No relevant text found in the database.\")\n",
    "        return\n",
    "    \n",
    "    retrieved_text = \"\\n\".join(retrieved_texts)\n",
    "    print(f\"üîç Retrieved Text:\\n{retrieved_text}\\n\")\n",
    "    \n",
    "    # Append user query to chat history\n",
    "    chat_history.add_message(\n",
    "        ChatMessageContent(\n",
    "            role=AuthorRole.USER,\n",
    "            items=[TextContent(text=user_input)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt_text = f\"\"\"Chat History:\n",
    "    {chat_history}\n",
    "    \n",
    "    Answer the following question based on the provided text within 30 words: \n",
    "    {retrieved_text}\n",
    "    \n",
    "    Question: {user_input}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    # Create prompt template configuration\n",
    "    prompt_template = PromptTemplateConfig(\n",
    "        template=prompt_text,\n",
    "        name=\"Test1\",\n",
    "        template_format=\"semantic-kernel\",\n",
    "        execution_settings=prompt_setting\n",
    "    )\n",
    "    \n",
    "    # Register the prompt function\n",
    "    prompt_function = kernel.add_function(\n",
    "        function_name=\"prompt_function\",\n",
    "        plugin_name=\"semantic_kernel_plugin\",\n",
    "        prompt_template_config=prompt_template\n",
    "    )\n",
    "    \n",
    "    # Wrap the arguments in KernelArguments\n",
    "    arguments = KernelArguments(\n",
    "        retrieved_text=retrieved_text,\n",
    "        user_query=user_input\n",
    "    )\n",
    "    \n",
    "    print(\"üìå Arguments: \", arguments)\n",
    "    print(\"üìå Prompt Function: \", prompt_function)\n",
    "    \n",
    "    # Invoke the model\n",
    "    result = await kernel.invoke(prompt_function, arguments=arguments)\n",
    "    \n",
    "    # Ensure the response is a string\n",
    "    ai_response = str(result) if not isinstance(result, str) else result\n",
    "\n",
    "    # Append model response to chat history\n",
    "    chat_history.add_message(\n",
    "        ChatMessageContent(\n",
    "            role=AuthorRole.ASSISTANT,\n",
    "            items=[TextContent(text=ai_response)]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ AI Response:\\n{ai_response}\\n\")\n",
    "\n",
    "# Run the loop to process user input until 'exit' is typed\n",
    "async def run_loop():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Exiting the chat. Goodbye!\")\n",
    "            break\n",
    "        await process_user_query(user_input)\n",
    "\n",
    "# Start the loop\n",
    "asyncio.run(run_loop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30327bf-c36b-42fa-a55e-e02b827f75c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2dc98229-1518-45c7-a5ff-868ae9b55c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/inkri/Semantic-Kernel/blob/main/SK_Quick_Start.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac693da0-df4f-4944-991c-e4341798dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/john0isaac/rag-semantic-kernel-mongodb-vcore/blob/main/rag-azure-openai-cosmosdb-notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01ef8f53-018a-4b71-8e2d-b7f243d77ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Azure-Samples/semantic-kernel-rag-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884af6b-c4f4-4b00-a72b-d064ee06053c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
